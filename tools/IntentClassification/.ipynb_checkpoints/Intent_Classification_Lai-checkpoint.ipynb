{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported dependencies.\n"
     ]
    }
   ],
   "source": [
    "# Import the dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import layers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "print(\"Imported dependencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions (also for importing)\n",
    "def readdata(filename):\n",
    "    return pd.read_csv(filename, names = [\"message\", \"Intent\"],encoding = \"latin1\")\n",
    "\n",
    "def cleaning(sentences):\n",
    "    words = []\n",
    "    for s in sentences:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        #stemming\n",
    "        words.append([i.lower() for i in w])\n",
    "    return words\n",
    "\n",
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    token = Tokenizer(filters = filters)\n",
    "    token.fit_on_texts(words)\n",
    "    return token\n",
    "\n",
    "def maximum_length(words):\n",
    "    return(len(max(words, key = len)))\n",
    "\n",
    "def encoding_doc(token, words):\n",
    "    return(token.texts_to_sequences(words))\n",
    "\n",
    "def padding_doc(encoded_doc, max_length):\n",
    "    return(pad_sequences(encoded_doc, maxlen = int(max_length), padding = \"post\"))\n",
    "\n",
    "def one_hot(encode):\n",
    "    o = OneHotEncoder(sparse = False)\n",
    "    return(o.fit_transform(encode))\n",
    "\n",
    "def create_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    #model.add(LSTM(128))\n",
    "    model.add(Dense(32, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(intentsize, activation = \"softmax\"))\n",
    "    return model\n",
    "\n",
    "def predictions(word_tokenizer, text, model, max_length=30):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "    #Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    "    x = padding_doc(test_ls, max_length)\n",
    "    pred = model.predict_proba(x)\n",
    "    return pred\n",
    "\n",
    "#Function to output predictions\n",
    "def get_final_output(pred, classes, mode):\n",
    "    predictions = pred[0]\n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    "    if mode == 'rank':\n",
    "        for i in range(pred.shape[1]):\n",
    "            print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "    elif mode == 'classify':\n",
    "        return classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Extract Dataset\n",
    "    filename = 'datasets/Preliminary_Dataset.csv'\n",
    "    trainingdata = readdata(filename)\n",
    "    messages = trainingdata['message']\n",
    "    intent = trainingdata['Intent']\n",
    "    unique_intent = list(set(intent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Process x values\n",
    "    #Cleaning messages, removing numbers and \n",
    "    words = cleaning(messages)\n",
    "\n",
    "    #Tokenizing messages\n",
    "    word_tokenizer = create_tokenizer(words)\n",
    "    #Saving tokenizer for other files\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(word_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    #Figure out vocabulary size\n",
    "    vocab_size = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "    #Find the max length of a message\n",
    "    max_length = maximum_length(words)\n",
    "    #print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))\n",
    "    #Save the max length to a file\n",
    "    with open('maxlen.txt', 'w') as file:\n",
    "        file.write(str(max_length))\n",
    "    \n",
    "    #Created a document that can be inputted in Keras\n",
    "    encoded_doc = encoding_doc(word_tokenizer,words)\n",
    "    padded_doc = pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\")\n",
    "    #print(padded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test and training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laitcl/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "    #Process Outputs\n",
    "    #Tokenize intentions\n",
    "    output_tokenizer=create_tokenizer(unique_intent,filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n",
    "    #output_tokenizer.word_index\n",
    "\n",
    "    #Number of unique intents\n",
    "    intentsize = len(unique_intent)\n",
    "\n",
    "    #Encode output for keras\n",
    "    encoded_output =  encoding_doc(output_tokenizer,intent)\n",
    "    encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)\n",
    "\n",
    "    #make onehot output\n",
    "    output_one_hot = one_hot(encoded_output)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size=0.2, random_state=1000)\n",
    "    #print(\"Shape of x_train = %s and y_train = %s\" % (x_train.shape, y_train.shape))\n",
    "    #print(\"Shape of x_test = %s and y_test = %s\" % (x_test.shape, y_test.shape))\n",
    "\n",
    "    print(\"Created test and training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/laitcl/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/laitcl/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 42, 128)           188544    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 460,068\n",
      "Trainable params: 271,524\n",
      "Non-trainable params: 188,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    #Model Creation\n",
    "    model = create_model(vocab_size, max_length)\n",
    "\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/laitcl/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2160 samples, validate on 540 samples\n",
      "Epoch 1/20\n",
      "2160/2160 [==============================] - 23s 11ms/step - loss: 1.2265 - acc: 0.4394 - val_loss: 1.0042 - val_acc: 0.5648\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00424, saving model to model_weights.h5\n",
      "Epoch 2/20\n",
      "2160/2160 [==============================] - 19s 9ms/step - loss: 0.9777 - acc: 0.5384 - val_loss: 0.8278 - val_acc: 0.5722\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00424 to 0.82776, saving model to model_weights.h5\n",
      "Epoch 3/20\n",
      "2160/2160 [==============================] - 18s 8ms/step - loss: 0.8655 - acc: 0.6097 - val_loss: 0.7177 - val_acc: 0.6852\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.82776 to 0.71767, saving model to model_weights.h5\n",
      "Epoch 4/20\n",
      "2160/2160 [==============================] - 18s 8ms/step - loss: 0.7636 - acc: 0.6597 - val_loss: 0.6552 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.71767 to 0.65523, saving model to model_weights.h5\n",
      "Epoch 5/20\n",
      "2160/2160 [==============================] - 18s 8ms/step - loss: 0.6932 - acc: 0.6926 - val_loss: 0.6087 - val_acc: 0.7370\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65523 to 0.60871, saving model to model_weights.h5\n",
      "Epoch 6/20\n",
      "2160/2160 [==============================] - 20s 9ms/step - loss: 0.6355 - acc: 0.7264 - val_loss: 0.5614 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.60871 to 0.56137, saving model to model_weights.h5\n",
      "Epoch 7/20\n",
      "2160/2160 [==============================] - 18s 9ms/step - loss: 0.5756 - acc: 0.7500 - val_loss: 0.5310 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.56137 to 0.53096, saving model to model_weights.h5\n",
      "Epoch 8/20\n",
      "2160/2160 [==============================] - 17s 8ms/step - loss: 0.5585 - acc: 0.7829 - val_loss: 0.5356 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53096\n",
      "Epoch 9/20\n",
      "2160/2160 [==============================] - 17s 8ms/step - loss: 0.5163 - acc: 0.7949 - val_loss: 0.4973 - val_acc: 0.8315\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.53096 to 0.49727, saving model to model_weights.h5\n",
      "Epoch 10/20\n",
      "2160/2160 [==============================] - 17s 8ms/step - loss: 0.5057 - acc: 0.8153 - val_loss: 0.5019 - val_acc: 0.8204\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49727\n",
      "Epoch 11/20\n",
      "2160/2160 [==============================] - 17s 8ms/step - loss: 0.4783 - acc: 0.8227 - val_loss: 0.4938 - val_acc: 0.8074\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49727 to 0.49378, saving model to model_weights.h5\n",
      "Epoch 12/20\n",
      "2160/2160 [==============================] - 18s 8ms/step - loss: 0.4791 - acc: 0.8157 - val_loss: 0.4726 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49378 to 0.47258, saving model to model_weights.h5\n",
      "Epoch 13/20\n",
      "2160/2160 [==============================] - 20s 9ms/step - loss: 0.4618 - acc: 0.8324 - val_loss: 0.4647 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.47258 to 0.46469, saving model to model_weights.h5\n",
      "Epoch 14/20\n",
      "2160/2160 [==============================] - 18s 8ms/step - loss: 0.4123 - acc: 0.8449 - val_loss: 0.4534 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.46469 to 0.45338, saving model to model_weights.h5\n",
      "Epoch 15/20\n",
      "2160/2160 [==============================] - 19s 9ms/step - loss: 0.4254 - acc: 0.8495 - val_loss: 0.4586 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.45338\n",
      "Epoch 16/20\n",
      "2160/2160 [==============================] - 20s 9ms/step - loss: 0.4180 - acc: 0.8569 - val_loss: 0.4409 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.45338 to 0.44091, saving model to model_weights.h5\n",
      "Epoch 17/20\n",
      "2160/2160 [==============================] - 19s 9ms/step - loss: 0.4111 - acc: 0.8551 - val_loss: 0.4833 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44091\n",
      "Epoch 18/20\n",
      "2160/2160 [==============================] - 19s 9ms/step - loss: 0.3976 - acc: 0.8653 - val_loss: 0.4646 - val_acc: 0.8574\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.44091\n",
      "Epoch 19/20\n",
      "2160/2160 [==============================] - 18s 8ms/step - loss: 0.3766 - acc: 0.8704 - val_loss: 0.4515 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.44091\n",
      "Epoch 20/20\n",
      "2160/2160 [==============================] - 20s 9ms/step - loss: 0.3870 - acc: 0.8685 - val_loss: 0.4878 - val_acc: 0.8315\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.44091\n"
     ]
    }
   ],
   "source": [
    "    # Fit the model to the training data\n",
    "    filename = 'model_weights.h5'\n",
    "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    hist = model.fit(x_train, y_train, epochs = 20, batch_size = 32, validation_data = (x_test, y_test), callbacks = [checkpoint])\n",
    "    \n",
    "    model.save_weights('model_weights.h5')\n",
    "    # Save the model architecture\n",
    "    with open('model_architecture.json', 'w') as f:\n",
    "        f.write(model.to_json())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Load our proudly trained model\n",
    "    with open('model_architecture.json', 'r') as f1:\n",
    "        model = model_from_json(f1.read())\n",
    "\n",
    "    # Load weights into the new model\n",
    "    model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disappointment'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #Example text\n",
    "    text = \"f\"\n",
    "    #Prediction\n",
    "    pred = predictions(word_tokenizer, text, model, max_length)\n",
    "    get_final_output(pred, unique_intent, 'classify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Intent_Classification_Lai.ipynb to script\n",
      "[NbConvertApp] Writing 6088 bytes to Intent_Classification_Lai.py\n"
     ]
    }
   ],
   "source": [
    "    #Export this notebook as a script\n",
    "    #!jupyter nbconvert --to script Intent_Classification_Lai.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
